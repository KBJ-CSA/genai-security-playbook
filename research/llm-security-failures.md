# LLM Security Failures — Real-World Incidents & Lessons

This document summarizes notable real-world failures involving Large Language Models (LLMs), analyzing root causes, attack patterns, and defensive takeaways.

---

## 1. Data Leakage & Confidentiality Incidents

### Samsung Internal Code Exposure (2023)

**Summary**  
Employees pasted sensitive source code into an LLM assistant, resulting in potential retention in model logs.

**Root Cause**
- Lack of data loss prevention (DLP)
- No enterprise usage policy enforcement

**Attack Pattern**
Unintentional insider data exfiltration

**Security Lessons**
- LLM usage must be governed like any external SaaS
- Prompt inputs are data egress channels

**Mitigations**
- Enterprise LLM gateways
- Prompt redaction and monitoring
- Clear acceptable use policies

---

### Chat History Exposure Bug (2023)

**Summary**  
A platform bug allowed some users to briefly see titles of other users’ conversations.

**Root Cause**
- Multi-tenant isolation flaw

**Attack Pattern**
Cross-tenant data exposure

**Security Lessons**
- LLM SaaS must follow strict tenant isolation
- Conversation metadata is sensitive

**Mitigations**
- Secure caching layers
- Strong access controls
- Continuous security testing

---

## 2. Legal & Liability Failures

### Fake Case Citations Submitted in Court (2023)

**Summary**  
A lawyer submitted hallucinated legal citations generated by an LLM.

**Root Cause**
- Overreliance on unverified model output

**Attack Pattern**
Hallucination exploitation

**Security Lessons**
- LLM output cannot be treated as authoritative
- Human verification is mandatory in high-risk domains

**Mitigations**
- Output validation workflows
- Source attribution enforcement

---

### Airline Chatbot Liability Ruling (2024)

**Summary**  
A court ruled an airline responsible for incorrect refund policy advice given by its chatbot.

**Root Cause**
- Chatbot treated as official representative

**Attack Pattern**
Policy misrepresentation

**Security Lessons**
- LLM outputs can create legal obligations
- Disclaimers alone are insufficient

**Mitigations**
- Retrieval-grounded responses
- Policy approval pipelines

---

## 3. Alignment & Abuse Incidents

### Public Chatbot Manipulation (Early Example)

**Summary**  
A public chatbot was manipulated into producing harmful content through adversarial prompts.

**Root Cause**
- Lack of robust adversarial training

**Attack Pattern**
Prompt manipulation

**Security Lessons**
- Public-facing LLMs require strong safety layers
- Continuous red teaming is essential

**Mitigations**
- Safety fine-tuning
- Real-time content filters
- Abuse monitoring

---

## Key Takeaways

1. Prompts are a new attack surface  
2. LLMs create legal and compliance risk  
3. Memory and context introduce persistence threats  
4. Human-in-the-loop validation remains critical  

---

## Recommended Controls

- Prompt filtering & inspection
- Output risk scoring
- RAG source validation
- Memory isolation
- Continuous red teaming
