# LLM Red Teaming Resources

A curated collection of tools, labs, research, and frameworks for learning and practicing LLM red teaming.

---

# ğŸ§ª Practice Targets & Labs

Hands-on environments to practice prompt injection and jailbreaks.

- Folly â€” Prompt injection playground  
- DamnVulnerableMathLLM â€” Intentionally vulnerable math LLM  
- DamnVulnerableShoppingLLM â€” Data leakage and tool misuse scenarios  
- RedTeam Arena â€” Competitive jailbreak challenges  
- MyLLM Bank CTF â€” Multi-LLM exploitation lab  
- Gandalf â€” Secret extraction challenge  

---

# ğŸ› ï¸ Red Teaming Frameworks

Tools for systematic and automated testing.

- PyRIT â€” Automated LLM red teaming framework  
- Rigging â€” Agent orchestration harness  

---

# âš”ï¸ Attack & Jailbreak Toolkits

For generating adversarial prompts and evaluating robustness.

- BrokenHill â€” GCG-based jailbreak attacks  
- llm-attacks â€” Universal adversarial suffix research code  
- Spikee â€” Prompt injection probing toolkit  

---

# ğŸ•µï¸ Traffic Interception & Analysis

Useful for black-box testing and prompt inspection.

- LLM-itM â€” MITM proxy for LLM traffic  

---

# ğŸ” Obfuscation Techniques

Used to test filter bypass scenarios.

- Unicode invisible characters  
- Emoji encoding  
- Text obfuscation tools  

---

# ğŸ›¡ï¸ Defensive Frameworks & Standards

Baseline controls to evaluate security posture.

- OWASP Top 10 for LLMs  
- Constitutional AI classifiers  
- Prompt guard models  

---

# ğŸ“„ Research Areas

Key research directions in LLM security:

- Universal jailbreak attacks  
- Adversarial prompt optimization  
- Context poisoning  
- Model extraction risks  
- Guardrail bypass techniques  

---

# ğŸ§© Case Studies & Incidents

Real-world examples of LLM security failures.

- Guardrail bypass evolution  
- Multi-chain prompt injection attacks  
- Context pollution attacks  
- Chatbot legal incidents  

---

